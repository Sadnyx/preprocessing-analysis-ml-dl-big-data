
UYGULAMA-1  (mysql -> sqoop ->  hdfs)
=======================================
Bu uygulamada mysql veri tabanında bir tablo oluşturularak iris verisi bu tabloya yazılacak,
sqoop ile bu tablo okunup hdfs'e aktarılacaktır.

1. mysql veri tabanına yüklenecek iris.csv dosyasını lokal diske indirme:
[root@sandbox-hdp ~]# wget https://raw.githubusercontent.com/erkansirin78/datasets/master/iris.csv

2. mysql veri tabanında bir veri tabanı ve tablo yaratma 
	2.1. mysql veri tabanına shell ile bağlanma:
	[root@sandbox-hdp ~]# mysql -u root -p
		Şifre: hadoop
		
	2.2. yeni bir veri tabanı yaratma:
	[root@sandbox-hdp ~]# create database azhadoop;
	
	2.3. Mevcut veri tabanlarını görüntüleme:
	mysql> show databases;
		+--------------------+
		| Database           |
		+--------------------+
		| information_schema |
		| azhadoop           |
		| hive               |
		| mysql              |
		| performance_schema |
		| ranger             |
		+--------------------+
		6 rows in set (0.04 sec)
		
	2.4. azhadoop vertabanına yetki
mysql> GRANT ALL PRIVILEGES ON azhadoop.* TO 'root'@'localhost';
mysql> GRANT ALL PRIVILEGES ON azhadoop.* TO 'root'@'sandbox-hdp.hortonworks.com';

	2.5. Kullanılacak veri tabanını seçme:
	mysql> use azhadoop;
	Database changed
	
	2.6. Veri tabanındaki tabloları listeleme:
	mysql> show tables;
	Empty set (0.00 sec)
	
	boş
	
	
3. iris.csv doyasının içeriği bu tabloya aktarma

	3.1. iris veri setine uygun tablo yaratma 
	
	mysql> create table iris_mysql(SepalLengthCm double, SepalWidthCm double, PetalLengthCm double, PetalWidthCm double, Species VARCHAR(20));

	mysql>
LOAD DATA LOCAL INFILE '/root/iris.csv' INTO TABLE iris_mysql
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 ROWS
(SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species);

Yükleme kontrolü:
mysql> select * from iris_mysql limit 5;
+---------------+--------------+---------------+--------------+-------------+
| SepalLengthCm | SepalWidthCm | PetalLengthCm | PetalWidthCm | Species     |
+---------------+--------------+---------------+--------------+-------------+
|           5.1 |          3.5 |           1.4 |          0.2 | Iris-setosa |
|           4.9 |            3 |           1.4 |          0.2 | Iris-setosa |
|           4.7 |          3.2 |           1.3 |          0.2 | Iris-setosa |
|           4.6 |          3.1 |           1.5 |          0.2 | Iris-setosa |
|             5 |          3.6 |           1.4 |          0.2 | Iris-setosa |
+---------------+--------------+---------------+--------------+-------------+
5 rows in set (0.00 sec)


mysql çıkış.
 \q:


3. sqoop eval komutu

[root@sandbox-hdp ~]# sqoop eval --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--username root --password hadoop --query "select * from iris_mysql limit 5"

Bu komutun sonunda ekrana sorgu sonucu gelmelidir.


Opsiyonel madde!
5. 
java.lang.ClassNotFoundException: org.apache.atlas.sqoop.hook.SqoopHook
 hatası alındığında Ambari'den Atlas servisini silelim çünkü lib ler çakışıyor. 
Denemelerde alınan hatalardan Atlas hook sürekli Sqoop'un çalışmasını engellediği görüldü.

6. sqoop import ile mysql'den hdfs'e veri aktarma 
	
	6.1. import edilecek dizini hdfs'te yaratalım:
		[maria_dev@sandbox-hdp ~]$ hdfs dfs -mkdir /user/maria_dev/sqoop_import

		Sqoop import'u çalştıralım:

[maria_dev@sandbox-hdp ~]$ sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--table iris_mysql --m 1 --target-dir /user/maria_dev/sqoop_import/iris

Hedef dizinde kontrol yapalım:
Ambari FilesView ile /user/maria_dev/sqoop_import/iris
diznini kontrol edelim ve iris verisetini görelim.
part-m-00000

Yukarıda --m 1 parametresini mapping sayısı için kullanıyoruz. Şayet tablomuzda artan sıralı bir anahtar olsaydı
bunu daha fazla parça ile yapabilirdik. Ancak olmadığı için mecbur --m 1 yapıyoruz.








UYGULAMA-2  (mysql -> sqoop ->  Hive)
=======================================

7. sqoop import ile mysql'den Hive'a veri aktarma ön ayarlar:

Ambari -> MapReduce -> Configs ->  Custom mapred-site  -> Add Property
mapreduce.job.ubertask.enable = true
mapreduce.job.ubertask.maxmaps = 1
mapreduce.job.ubertask.maxreduces = 1
mapreduce.job.ubertask.maxbytes = 134217728

mapreduce.job.ubertask.maxbytes değeri HDFS ayarlarından alındı dfs.block.size=134217728


Hive içinde CREATE TABLE IF NOT EXISTS azhadoop

8. Aktarım:

8.1. hdfs kullanıcısına geçme:

su hdfs


8.2. Varsa hedef dizin ve hive tablosunu silme
target dir silme:
hdfs dfs -rm -R -skipTrash /tmp/hive_temp
hedef tablo silme:
DROP TABLE iris_mysql;

8.3. Hive tablosu mevcut değilse:
----------------------------------
sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--query "select * from iris_mysql WHERE \$CONDITIONS" \
--m 1 --hive-import --create-hive-table \
--hive-table azhadoop.iris_hive --target-dir /tmp/hive_temp


8.4. Hive tablosu mevcut ve üzerine yazılacaksa:
-----------------------------------------------

8.4.1. mysql'den iris_mysql tablosuna bir satır ekleyelim:

	[hdfs@sandbox-hdp root]$ mysql -u root -p
	mysql> use azhadoop
	mysql> insert into iris_mysql values(6.5,3.0,5.2,2.0,"Iris-virginica");
	\q;
8.4.2. üzerine yazarak hive'a aktarma:
	
sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--query "select * from iris_mysql WHERE \$CONDITIONS" \
--m 1 --hive-import --hive-overwrite \
--hive-table azhadoop.iris_hive --target-dir /tmp/hive_temp


hive tablosundan eklen satırı kontrol edebilirsiniz.
