
UYGULAMA-1  (mysql -> sqoop ->  hdfs)
=======================================
Bu uygulamada mysql veri tabanında bir tablo oluşturularak iris verisi bu tabloya yazılacak,
sqoop ile bu tablo okunup hdfs'e aktarılacaktır.

1. mysql veri tabanına yüklenecek iris.csv dosyasını lokal diske indirme:
[root@sandbox-hdp ~]# wget https://raw.githubusercontent.com/erkansirin78/datasets/master/iris.csv

2. mysql veri tabanında bir veri tabanı ve tablo yaratma 
	2.1. mysql veri tabanına shell ile bağlanma:
	[root@sandbox-hdp ~]# mysql -u root -p
		Şifre: hadoop
		
	2.2. yeni bir veri tabanı yaratma:
	[root@sandbox-hdp ~]# create database azhadoop;
	
	2.3. Mevcut veri tabanlarını görüntüleme:
	mysql> show databases;
		+--------------------+
		| Database           |
		+--------------------+
		| information_schema |
		| azhadoop           |
		| hive               |
		| mysql              |
		| performance_schema |
		| ranger             |
		+--------------------+
		6 rows in set (0.04 sec)
		
	2.4. azhadoop vertabanına yetki
mysql> GRANT ALL PRIVILEGES ON azhadoop.* TO 'root'@'localhost';
mysql> GRANT ALL PRIVILEGES ON azhadoop.* TO 'root'@'sandbox-hdp.hortonworks.com';

	2.5. Kullanılacak veri tabanını seçme:
	mysql> use azhadoop;
	Database changed
	
	2.6. Veri tabanındaki tabloları listeleme:
	mysql> show tables;
	Empty set (0.00 sec)
	
	boş
	
	
3. iris.csv doyasının içeriği bu tabloya aktarma

	3.1. iris veri setine uygun tablo yaratma 
	
	mysql> create table iris_mysql(SepalLengthCm double, SepalWidthCm double, PetalLengthCm double, PetalWidthCm double, Species VARCHAR(20));

	mysql>
LOAD DATA LOCAL INFILE '/root/iris.csv' INTO TABLE iris_mysql
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 ROWS
(SepalLengthCm,SepalWidthCm,PetalLengthCm,PetalWidthCm,Species);

Yükleme kontrolü:
mysql> select * from iris_mysql limit 5;
+---------------+--------------+---------------+--------------+-------------+
| SepalLengthCm | SepalWidthCm | PetalLengthCm | PetalWidthCm | Species     |
+---------------+--------------+---------------+--------------+-------------+
|           5.1 |          3.5 |           1.4 |          0.2 | Iris-setosa |
|           4.9 |            3 |           1.4 |          0.2 | Iris-setosa |
|           4.7 |          3.2 |           1.3 |          0.2 | Iris-setosa |
|           4.6 |          3.1 |           1.5 |          0.2 | Iris-setosa |
|             5 |          3.6 |           1.4 |          0.2 | Iris-setosa |
+---------------+--------------+---------------+--------------+-------------+
5 rows in set (0.00 sec)


mysql çıkış.
 \q:


3. sqoop eval komutu

[root@sandbox-hdp ~]# sqoop eval --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--username root --password hadoop --query "select * from iris_mysql limit 5"

Bu komutun sonunda ekrana sorgu sonucu gelmelidir.


Opsiyonel madde!
5. 
java.lang.ClassNotFoundException: org.apache.atlas.sqoop.hook.SqoopHook
 hatası alındığında Ambari'den Atlas servisini silelim çünkü lib ler çakışıyor. 
Denemelerde alınan hatalardan Atlas hook sürekli Sqoop'un çalışmasını engellediği görüldü.
Atlas kaldırmadan devam etmek için:
https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.3/bk_command-line-installation/content/configuring-atlas-sqoop-hook.html

6. sqoop import ile mysql'den hdfs'e veri aktarma 
	
	6.1. import edilecek dizini hdfs'te yaratalım:
		[maria_dev@sandbox-hdp ~]$ hdfs dfs -mkdir /user/maria_dev/sqoop_import

		Sqoop import'u çalştıralım:

[maria_dev@sandbox-hdp ~]$ sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--table iris_mysql --m 1 --target-dir /user/maria_dev/sqoop_import/iris

Hedef dizinde kontrol yapalım:
Ambari FilesView ile /user/maria_dev/sqoop_import/iris
diznini kontrol edelim ve iris verisetini görelim.
part-m-00000

Yukarıda --m 1 parametresini mapping sayısı için kullanıyoruz. Şayet tablomuzda artan sıralı bir anahtar olsaydı
bunu daha fazla parça ile yapabilirdik. Ancak olmadığı için mecbur --m 1 yapıyoruz.








UYGULAMA-2  (mysql -> sqoop ->  Hive)
=======================================

7. sqoop import ile mysql'den Hive'a veri aktarma ön ayarlar:

Ambari -> MapReduce -> Configs ->  Custom mapred-site  -> Add Property
mapreduce.job.ubertask.enable = true
mapreduce.job.ubertask.maxmaps = 1
mapreduce.job.ubertask.maxreduces = 1
mapreduce.job.ubertask.maxbytes = 134217728

mapreduce.job.ubertask.maxbytes değeri HDFS ayarlarından alındı dfs.block.size=134217728


Hive içinde CREATE TABLE IF NOT EXISTS azhadoop

8. Aktarım:

8.1. hdfs kullanıcısına geçme:

su hdfs


8.2. Varsa hedef dizin ve hive tablosunu silme
target dir silme:
hdfs dfs -rm -R -skipTrash /tmp/hive_temp
hedef tablo silme:
DROP TABLE iris_mysql;

8.3. Hive tablosu mevcut değilse:
----------------------------------
sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--query "select * from iris_mysql WHERE \$CONDITIONS" \
--m 1 --hive-import --create-hive-table \
--hive-table azhadoop.iris_hive --target-dir /tmp/hive_temp


8.4. Hive tablosu mevcut ve üzerine yazılacaksa:
-----------------------------------------------

8.4.1. mysql'den iris_mysql tablosuna bir satır ekleyelim:

	[hdfs@sandbox-hdp root]$ mysql -u root -p
	mysql> use azhadoop
	mysql> insert into iris_mysql values(6.5,3.0,5.2,2.0,"Iris-virginica");
	\q;
8.4.2. üzerine yazarak hive'a aktarma:
	
sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--query "select * from iris_mysql WHERE \$CONDITIONS" \
--m 1 --hive-import --hive-overwrite \
--hive-table azhadoop.iris_hive --target-dir /tmp/hive_temp


hive tablosundan eklenen satırı kontrol edebilirsiniz.











UYGULAMA-3  (postgresql -> sqoop ->  Hive)
==========================================

1. Docker machine çalıştırma (toolbox version)
docker-machine start default 
docker-machine env
eval $("C:\Program Files\Docker Toolbox\docker-machine.exe" env)
Not: Yukarıdaki komut sizin ekranınızda en alt satırda bulunur. Onu kopyalayıp yapıştırın.

2. POSTGRESQL Docker Kurulumu (kurulu ise bu adımı atlayın)
docker run -p 5432:5432 -d \
-e POSTGRES_PASSWORD=postgres \
-e POSTGRES_USER=postgres \
-e POSTGRES_DB=spark \
-v pgdata:/var/lib/postgresql/data \
postgres



3. posgre container id öğrenme
docker ps
veya
docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES
<container_id>        postgres            "docker-entrypoint.s…"   14 seconds ago      Up 14 seconds       0.0.0.0:5432->5432/tcp   nervous_leavitt

4. 
winpty docker.exe exec -it <container_id> psql -U postgres spark


Beklenen sonuç:
psql (11.2 (Debian 11.2-1.pgdg90+1))
Type "help" for help.

spark=#

5. Advertising veri seti için tablo oluştur.
mevcut veri tabanlarını listele:
postgres-# \l

spark veri tabanını seç 
postgres=# \c spark


CREATE TABLE public.advertising (
id int4 NULL,
tv float8 NULL,
radio float8 NULL,
newspaper float8 NULL,
sales float8 NULL
);


6. postgresql shell çıkış 
\q


7. Ana makineden postgres container içine csv dosyası kopyalama 
Ana makineden posgresql e dosya kopyalama
veri dosyasının olduğu dizinde:

$ docker cp Advertising.csv <container_id>:/Advertising.csv

8.
pip install psycopg2

9. postgres kullanıcısı ile postgres shelle bağlanma:
winpty docker.exe exec -it cea psql -U postgres
 
Veri tabanlarını listeleme:
 postgres-# \l
 spark veri tabanını seç 
 postgres=# \c spark
 
10. veriyi tabloya yazma:
COPY advertising FROM '/Advertising.csv' DELIMITERS ',' CSV HEADER;

 sonuç: 
COPY 200



11. postgresql driver indir 
[root@sandbox-hdp ~]# wget https://jdbc.postgresql.org/download/postgresql-42.2.6.jar


12. jar'ı sqoop library'e taşı
[root@sandbox-hdp ~]# cp postgresql-42.2.6.jar /usr/hdp/current/sqoop-client/lib/


13. sqoop eval ile erişimi kontrol et
[root@sandbox-hdp ~]# sqoop eval --connect jdbc:postgresql://192.168.99.107:5432/spark \
> --username postgres --password postgres --query "select * from advertising limit 5"

Sonuç:
Warning: /usr/hdp/2.6.4.0-91/accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
19/07/24 02:53:50 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6.2.6.4.0-91
19/07/24 02:53:50 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
19/07/24 02:53:50 INFO manager.SqlManager: Using default fetchSize of 1000
-----------------------------------------------------------------------------------------------------------
| id          | tv                   | radio                | newspaper            | sales                |
-----------------------------------------------------------------------------------------------------------
| 1           | 230.099999999999994  | 37.7999999999999972  | 69.2000000000000028  | 22.1000000000000014  |
| 2           | 44.5                 | 39.2999999999999972  | 45.1000000000000014  | 10.4000000000000004  |
| 3           | 17.1999999999999993  | 45.8999999999999986  | 69.2999999999999972  | 9.30000000000000071  |
| 4           | 151.5                | 41.2999999999999972  | 58.5                 | 18.5                 |
| 5           | 180.800000000000011  | 10.8000000000000007  | 58.3999999999999986  | 12.9000000000000004  |
-----------------------------------------------------------------------------------------------------------



13. Hive tablosu mevcut değilse:
----------------------------------
sqoop import --connect jdbc:postgresql://192.168.99.107:5432/spark \
--driver org.postgresql.Driver \
--username postgres --password postgres \
--query "select * from advertising WHERE \$CONDITIONS" \
--m 4 --split-by id --hive-import --create-hive-table \
--hive-table azhadoop.advertising --target-dir /tmp/hive_temp








8.4.2. üzerine yazarak hive'a aktarma:
	
sqoop import --connect jdbc:postgresql://192.168.99.107:5432/spark \
--driver org.postgresql.Driver \
--username postgres --password postgres \
--query "select * from advertising WHERE \$CONDITIONS" \
--m 4 --split-by id --hive-import --hive-overwrite \
--hive-table azhadoop.advertising --target-dir /tmp/hive_temp